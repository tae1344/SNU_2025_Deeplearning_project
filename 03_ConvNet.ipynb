{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from common.config import (\n",
    "    BASE_DIR,\n",
    "    NUM_CLASSES,\n",
    "    BATCH_SIZE,\n",
    "    EPOCHS,\n",
    "    WARMUP_EPOCHS,\n",
    "    SEED,\n",
    "    BASE_LEARNING_RATE,\n",
    "    WD,\n",
    "    SEED,\n",
    "    DEVICE,\n",
    "    WORKER_NUM,\n",
    "    CLASS_NAMES,\n",
    "    TRANSFORM,\n",
    "    TRAIN_TRANSFORM,\n",
    "    VAL_TRANSFORM,\n",
    "    TRAIN_IMAGE_DIR,\n",
    "    TRAIN_LABEL_DIR,\n",
    "    VAL_IMAGE_DIR,\n",
    "    VAL_LABEL_DIR,\n",
    "    TEST_IMAGE_DIR,\n",
    "    TEST_LABEL_DIR,\n",
    "    LOG_DIR,\n",
    "    TRAIN_LOG_DIR,\n",
    "    TEST_LOG_DIR,\n",
    "    MODEL_DIR,\n",
    "    DROPOUT_RATE,\n",
    ")\n",
    "from common.utils import (\n",
    "    set_seed,\n",
    "    worker_init_fn,\n",
    "    get_mean_std_from_weights,\n",
    "    get_device,\n",
    "    device_pretty,\n",
    "    human_time,\n",
    "    count_parameters,\n",
    "    calculate_top_k_accuracy,\n",
    "    calculate_f1_score,\n",
    "    calculate_auroc,\n",
    "    save_auroc_data,\n",
    "    save_checkpoint,\n",
    ")\n",
    "from common.dataset import CustomDataset\n",
    "from common.logger import Logger\n",
    "from common.evaluate import evaluate_test_set, print_test_results\n",
    "\n",
    "# initail seed 설정\n",
    "set_seed(SEED)\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Set device for computations\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"LABEL_DIR:\", TRAIN_LABEL_DIR)\n",
    "print(\"IMAGE_DIR:\", TRAIN_IMAGE_DIR)\n",
    "print(\"LABEL_DIR exists:\", os.path.isdir(TRAIN_LABEL_DIR))\n",
    "print(\"IMAGE_DIR exists:\", os.path.isdir(TRAIN_IMAGE_DIR))\n",
    "\n",
    "json_paths = glob.glob(os.path.join(TRAIN_LABEL_DIR, \"**\", \"*.json\"), recursive=True)\n",
    "print(\"Found json files:\", len(json_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd729a41",
   "metadata": {},
   "source": [
    "# 1. 데이터 셋 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE_NAME = \"best_convnet_model.pth\"\n",
    "\n",
    "# 1) 데이터 증강,변환(색 왜곡은 과하지 않게)\n",
    "train_dataset = CustomDataset(\n",
    "    label_folder=TRAIN_LABEL_DIR, image_folder=TRAIN_IMAGE_DIR, transform=TRAIN_TRANSFORM\n",
    ")\n",
    "val_datatset = CustomDataset(\n",
    "    label_folder=VAL_LABEL_DIR, image_folder=VAL_IMAGE_DIR, transform=VAL_TRANSFORM\n",
    ")\n",
    "test_datatset = CustomDataset(\n",
    "    label_folder=TEST_LABEL_DIR, image_folder=TEST_IMAGE_DIR, transform=VAL_TRANSFORM\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=WORKER_NUM,\n",
    "    generator=torch.Generator(device=DEVICE),\n",
    "    worker_init_fn=worker_init_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_datatset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=WORKER_NUM,\n",
    "    generator=torch.Generator(device=DEVICE),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    val_datatset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=WORKER_NUM,\n",
    "    generator=torch.Generator(device=DEVICE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607b3b7",
   "metadata": {},
   "source": [
    "# 2. 모델 설정 - ConvNet\n",
    "\n",
    "## 각 모델별로 다르게 설정!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=15, dropout_rate=0.5):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # First block: Conv(3→64)→BN→ReLU→MaxPool\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Second block: [Conv(64→128)→BN→ReLU]×2→MaxPool\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Third block: [Conv(128→256)→BN→ReLU]×2→MaxPool\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        # He initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 모델: ConvNet - CNN 기반 모델\n",
    "def create_model():\n",
    "    model = ConvNet(num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE)  # 15, 0.5\n",
    "    return model.to(device)\n",
    "\n",
    "model = create_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c0710",
   "metadata": {},
   "source": [
    "# 3. Optima, Scheduler, Scaler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 손실/옵티마/스케줄러\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=BASE_LEARNING_RATE, weight_decay=WD\n",
    ")\n",
    "\n",
    "def lr_lambda(current_epoch):\n",
    "    if current_epoch < WARMUP_EPOCHS:\n",
    "        return float(current_epoch + 1) / WARMUP_EPOCHS\n",
    "    # cosine\n",
    "    t = (current_epoch - WARMUP_EPOCHS) / max(1, (EPOCHS - WARMUP_EPOCHS))\n",
    "    return 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "# 스케쥴러\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 스케일러\n",
    "# Mixed Precision Training 설정 (GPU에서만 사용)\n",
    "if torch.cuda.is_available():\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    print(\"✅ Mixed Precision Training enabled (GPU)\")\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"⚠️  Mixed Precision Training disabled (CPU mode)\")\n",
    "\n",
    "# Logger 설정\n",
    "logger = Logger(LOG_DIR, \"train_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ea90a",
   "metadata": {},
   "source": [
    "# 4. 평가지표 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "train_top3_accuracies, val_top3_accuracies = [], []\n",
    "train_aurocs, val_aurocs = [], []\n",
    "train_f1_scores, val_f1_scores = [], []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_composite_score = 0.0  # 복합 점수 초기화\n",
    "best_epoch = 0\n",
    "early_stopping_counter = 0\n",
    "patience = 20\n",
    "\n",
    "# 학습 시작 시간\n",
    "total_start_time = time.time()\n",
    "print(\n",
    "    f\"Training started at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afec4bc",
   "metadata": {},
   "source": [
    "# 5. 모델 학습 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, train_loader, criterion, optimizer, device, scaler=None, desc=None\n",
    "):\n",
    "    \"\"\"한 에포크 학습\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_top3_correct = 0\n",
    "    epoch_total = 0\n",
    "\n",
    "    train_outputs = []\n",
    "    train_labels = []\n",
    "\n",
    "    iterator = train_loader\n",
    "    if desc is not None:\n",
    "        try:\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            iterator = tqdm(train_loader, desc=desc)\n",
    "        except Exception:\n",
    "            iterator = train_loader\n",
    "\n",
    "    for images, labels in iterator:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        batch_size = labels.size(0)\n",
    "        epoch_total += batch_size\n",
    "        epoch_correct += (preds == labels).sum().item()\n",
    "        epoch_top3_correct += calculate_top_k_accuracy(outputs, labels, k=3)\n",
    "\n",
    "        train_outputs.extend(outputs.detach().cpu().tolist())\n",
    "        train_labels.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "    epoch_loss /= max(1, len(train_loader))\n",
    "    epoch_acc = epoch_correct / max(1, epoch_total)\n",
    "    epoch_top3_acc = epoch_top3_correct / max(1, epoch_total)\n",
    "\n",
    "    return {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"accuracy\": epoch_acc,\n",
    "        \"top3_accuracy\": epoch_top3_acc,\n",
    "        \"outputs\": train_outputs,\n",
    "        \"labels\": train_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def validation_one_epoch(model, val_loader, criterion, device, desc=None):\n",
    "    \"\"\"한 에포크 검증\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_top3_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_outputs = []\n",
    "    val_labels = []\n",
    "\n",
    "    iterator = val_loader\n",
    "    if desc is not None:\n",
    "        try:\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            iterator = tqdm(val_loader, desc=desc)\n",
    "        except Exception:\n",
    "            iterator = val_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in iterator:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            batch_size = labels.size(0)\n",
    "            val_total += batch_size\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_top3_correct += calculate_top_k_accuracy(outputs, labels, k=3)\n",
    "\n",
    "            val_outputs.extend(outputs.detach().cpu().tolist())\n",
    "            val_labels.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "    val_top3_acc = val_top3_correct / max(1, val_total)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": val_acc,\n",
    "        \"top3_accuracy\": val_top3_acc,\n",
    "        \"outputs\": val_outputs,\n",
    "        \"labels\": val_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57c791",
   "metadata": {},
   "source": [
    "# 6. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    # ===============================================\n",
    "    # Training\n",
    "    # ===============================================\n",
    "    train_metrics = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        desc=f\"Training Epoch {epoch+1}/{EPOCHS}\",\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_metrics[\"loss\"])\n",
    "    train_accuracies.append(train_metrics[\"accuracy\"])\n",
    "    train_top3_accuracies.append(train_metrics[\"top3_accuracy\"])\n",
    "    train_f1_scores.append(\n",
    "        calculate_f1_score(\n",
    "            torch.tensor(train_metrics[\"outputs\"]),\n",
    "            torch.tensor(train_metrics[\"labels\"]),\n",
    "        )\n",
    "    )\n",
    "    train_aurocs.append(\n",
    "        calculate_auroc(\n",
    "            torch.tensor(train_metrics[\"outputs\"]),\n",
    "            torch.tensor(train_metrics[\"labels\"]),\n",
    "        )\n",
    "    )\n",
    "    save_auroc_data(\n",
    "        train_metrics[\"outputs\"],\n",
    "        train_metrics[\"labels\"],\n",
    "        epoch + 1,\n",
    "        \"train\",\n",
    "        TRAIN_LOG_DIR,\n",
    "    )\n",
    "\n",
    "    # ===============================================\n",
    "    # Validation\n",
    "    # ===============================================\n",
    "    val_metrics = validation_one_epoch(\n",
    "        model=model,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        desc=\"Validating\",\n",
    "    )\n",
    "\n",
    "    val_losses.append(val_metrics[\"loss\"])\n",
    "    val_accuracies.append(val_metrics[\"accuracy\"])\n",
    "    val_top3_accuracies.append(val_metrics[\"top3_accuracy\"])\n",
    "    val_f1_scores.append(\n",
    "        calculate_f1_score(\n",
    "            torch.tensor(val_metrics[\"outputs\"]), torch.tensor(val_metrics[\"labels\"])\n",
    "        )\n",
    "    )\n",
    "    val_aurocs.append(\n",
    "        calculate_auroc(\n",
    "            torch.tensor(val_metrics[\"outputs\"]), torch.tensor(val_metrics[\"labels\"])\n",
    "        )\n",
    "    )\n",
    "    save_auroc_data(\n",
    "        val_metrics[\"outputs\"], val_metrics[\"labels\"], epoch + 1, \"val\", TRAIN_LOG_DIR\n",
    "    )\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step()  # ReduceLROnPlateau 사용 시: scheduler.step(val_metrics[\"loss\"])\n",
    "\n",
    "    # 에포크 소요 시간 계산\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    # 에포크 결과 출력\n",
    "    print(\n",
    "        f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "        f\"Time: {epoch_duration:.2f}s, \"\n",
    "        f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Train Top-3: {train_top3_accuracies[-1]:.4f}, Train F1: {train_f1_scores[-1]:.4f}, \"\n",
    "        f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}, Val Top-3: {val_top3_accuracies[-1]:.4f}, Val F1: {val_f1_scores[-1]:.4f}\"\n",
    "    )\n",
    "\n",
    "    # 의료 이미지 분류에 최적화된 복합 점수 계산\n",
    "    composite_score = (\n",
    "        0.4 * val_accuracies[-1]  # 40% - 정확도 (가장 중요)\n",
    "        + 0.3 * val_aurocs[-1]  # 30% - AUROC (의료 분류에서 중요)\n",
    "        + 0.2 * val_f1_scores[-1]  # 20% - F1 Score (클래스 불균형 고려)\n",
    "        + 0.1 * (1 - val_losses[-1])  # 10% - Loss (낮을수록 좋음)\n",
    "    )\n",
    "\n",
    "    # 최고 성능 모델 저장\n",
    "    if composite_score > best_composite_score:\n",
    "        best_composite_score = composite_score\n",
    "        best_epoch = epoch + 1\n",
    "        early_stopping_counter = 0\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,  # 현재 에포크\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": train_losses[-1],\n",
    "                \"train_accuracy\": train_accuracies[-1],\n",
    "                \"train_top3_accuracy\": train_top3_accuracies[-1],\n",
    "                \"train_f1_score\": train_f1_scores[-1],\n",
    "                \"train_auroc\": train_aurocs[-1],\n",
    "                \"val_loss\": val_losses[-1],\n",
    "                \"val_accuracy\": val_accuracies[-1],\n",
    "                \"val_top3_accuracy\": val_top3_accuracies[-1],\n",
    "                \"val_f1_score\": val_f1_scores[-1],\n",
    "                \"val_auroc\": val_aurocs[-1],\n",
    "                \"composite_score\": composite_score,\n",
    "                \"epoch_duration\": epoch_duration,\n",
    "            },\n",
    "            MODEL_DIR,\n",
    "            MODEL_FILE_NAME,\n",
    "        )\n",
    "        print(f\"🎯 New best model saved! Composite Score: {composite_score:.4f}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. Best epoch was {best_epoch}.\")\n",
    "        break\n",
    "\n",
    "    # 로그 기록\n",
    "    logger.log(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_losses[-1],\n",
    "            \"train_accuracy\": train_accuracies[-1],\n",
    "            \"train_top3_accuracy\": train_top3_accuracies[-1],\n",
    "            \"train_f1_score\": train_f1_scores[-1],\n",
    "            \"train_auroc\": train_aurocs[-1],\n",
    "            \"val_loss\": val_losses[-1],\n",
    "            \"val_accuracy\": val_accuracies[-1],\n",
    "            \"val_top3_accuracy\": val_top3_accuracies[-1],\n",
    "            \"val_f1_score\": val_f1_scores[-1],\n",
    "            \"val_auroc\": val_aurocs[-1],\n",
    "            \"composite_score\": composite_score,\n",
    "            \"epoch_duration\": epoch_duration,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 저장\n",
    "logger.save()\n",
    "\n",
    "# 학습 종료 시간\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(\n",
    "    f\"Training completed at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_end_time))}\"\n",
    ")\n",
    "print(f\"Total training time: {total_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6f40e",
   "metadata": {},
   "source": [
    "# 7. Test 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(os.path.join(MODEL_DIR, MODEL_FILE_NAME), weights_only=False)\n",
    "\n",
    "# 체크포인트의 키들 확인\n",
    "print(\"Checkpoint keys:\", list(checkpoint.keys()))\n",
    "print(\"State dict keys (first 10):\", list(checkpoint[\"state_dict\"].keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 (기존 모델 정의 코드 필요)\n",
    "model = create_model()\n",
    "\n",
    "# 체크포인트에서 가중치 로드\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# 평가 실행\n",
    "results = evaluate_test_set(model, test_loader, criterion, device)\n",
    "print_test_results(results, \"ResNet-101\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taeya_python_env3.13",
   "language": "python",
   "name": "taeya_python_env3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
