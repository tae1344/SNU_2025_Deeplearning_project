{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac38d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoFeatureExtractor, ResNetForImageClassification\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"huggingface/cats-image\")\n",
    "# image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-101\")\n",
    "# model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-101\")\n",
    "\n",
    "# inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_label = logits.argmax(-1).item()\n",
    "# print(model.config.id2label[predicted_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f5134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d272685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f31c50",
   "metadata": {},
   "source": [
    "# 예시1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf6abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # 1. Load a pre-trained ResNet-101 model\n",
    "# # You can choose to use the default ImageNet1K_V2 weights for improved accuracy\n",
    "# # or IMAGENET1K_V1 for the original paper's weights.\n",
    "# model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# # Set the model to evaluation mode (important for inference)\n",
    "# model.eval()\n",
    "\n",
    "# # 2. Define image transformations for inference\n",
    "# # These transformations match the preprocessing used for the pre-trained ImageNet weights.\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(256),            # Resize the image to 256x256\n",
    "#     transforms.CenterCrop(224),        # Crop the center 224x224\n",
    "#     transforms.ToTensor(),             # Convert to PyTorch Tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize\n",
    "# ])\n",
    "\n",
    "# # 3. Prepare an example image (replace with your own image loading)\n",
    "# # For demonstration, we'll create a dummy image.\n",
    "# # In a real scenario, you would load an image using PIL: Image.open(\"your_image.jpg\")\n",
    "# dummy_image = Image.new('RGB', (256, 256), color = 'red')\n",
    "\n",
    "# # Apply transformations to the image\n",
    "# input_tensor = preprocess(dummy_image)\n",
    "# input_batch = input_tensor.unsqueeze(0) # Add a batch dimension\n",
    "\n",
    "# # 4. Perform inference\n",
    "# with torch.no_grad(): # Disable gradient calculations for inference\n",
    "#     output = model(input_batch)\n",
    "\n",
    "# # 5. Process the output (e.g., get top predictions)\n",
    "# # The output will be log-probabilities for 1000 ImageNet classes.\n",
    "# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "# print(\"Top 5 predicted categories and their probabilities:\")\n",
    "# for i in range(top5_prob.size(0)):\n",
    "#     print(f\"  Category ID: {top5_catid[i].item()}, Probability: {top5_prob[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284fa52",
   "metadata": {},
   "source": [
    "# 예시2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ddf471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_seed 42\n",
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n",
      "CUDA device count: 0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, math, time\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "NUM_CLASSES = 15 # 분류할 클래스 수\n",
    "IMG_SIZE = 224 # 입력 이미지의 해상도(256x256 픽셀), 마지막 10~20ep에 320으로 리핏 권장\n",
    "BATCH = 48 # 배치 크기\n",
    "EPOCHS = 3 # 에폭 수\n",
    "BASE_LR = 5e-4 # 기본 learing rate(0.0005)\n",
    "WD = 0.05 # 가중치 감쇠(weight decay)(0.05) - 과적화 방지를 위한 정규화 기법\n",
    "WARMUP_EPOCHS = 1 # 워밍업 에폭 수 - 학습률을 점진적으로 증가시크는 에폭 수, 학습 초기에 안정적인 학습을 위한 warmup 기법\n",
    "SEED = 42\n",
    "\n",
    "# TRAIN_DATA_DIR = \"/content/drive/MyDrive/서울대 교육과정/딥러닝_피부데이터_프로젝트/data/training/01.원천데이터\"   # 훈련 데이터 경로\n",
    "# VAL_DATA_DIR = \"/content/drive/MyDrive/서울대 교육과정/딥러닝_피부데이터_프로젝트/data/validation/01.원천데이터\"   # 검증 데이터 경로\n",
    "TRAIN_DATA_DIR = \"./data/training/01.원천데이터\"   # 훈련 데이터 경로\n",
    "VAL_DATA_DIR = \"./data/validation/01.원천데이터\"   # 검증 데이터 경로\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"모든 랜덤 시드를 설정하는 함수\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    print('set_seed', seed)\n",
    "    # torch.cuda.manual_seed_all(seed)  # 멀티 GPU 사용 시\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"각 워커 프로세스에서 동일한 시드 설정\"\"\"\n",
    "    worker_seed = torch.initial_seed() + worker_id\n",
    "    set_seed(SEED)\n",
    "\n",
    "# 시드 설정\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Set device for computations\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179c9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 변환(색 왜곡은 과하지 않게)\n",
    "mean = [0.485, 0.456, 0.406]   # ImageNet\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DATA_DIR, transform=train_tf)\n",
    "val_datatset = datasets.ImageFolder(VAL_DATA_DIR, transform=val_tf)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=0, generator=torch.Generator(device=DEVICE), worker_init_fn=worker_init_fn,)\n",
    "val_loader   = DataLoader(val_datatset, batch_size=BATCH, shuffle=False, num_workers=0, generator=torch.Generator(device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff63e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 모델: ResNet101 + 헤드 교체\n",
    "weights = models.ResNet101_Weights.IMAGENET1K_V2\n",
    "model = models.resnet101(weights=weights)\n",
    "in_feats = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# (옵션) 초반 5~10 에폭 백본 동결 후 해제\n",
    "FREEZE_WARMUP_EPOCHS = 5\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in list(model.children())[:-1]:     # fc 제외 백본\n",
    "    for q in p.parameters():\n",
    "        q.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100be3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) 손실/옵티마/스케줄러\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=BASE_LR, weight_decay=WD)\n",
    "\n",
    "def lr_lambda(current_epoch):\n",
    "    if current_epoch < WARMUP_EPOCHS:\n",
    "        return float(current_epoch + 1) / WARMUP_EPOCHS\n",
    "    # cosine\n",
    "    t = (current_epoch - WARMUP_EPOCHS) / max(1, (EPOCHS - WARMUP_EPOCHS))\n",
    "    return 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "# 스케쥴러\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 스케일러\n",
    "scaler = torch.amp.GradScaler(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85c588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum, n = 0.0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast(device_type=DEVICE, dtype=torch.float16):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0); n += x.size(0)\n",
    "            y_true.extend(y.detach().cpu().tolist())\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).detach().cpu().tolist())\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return loss_sum / n, f1\n",
    "\n",
    "best_f1, best_path = 0.0, \"resnet101_best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b7379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training...\n",
      "  Batch 10/250, Loss: 2.4955\n",
      "  Batch 20/250, Loss: 2.2827\n",
      "  Batch 30/250, Loss: 2.0888\n",
      "  Batch 40/250, Loss: 1.8946\n",
      "  Batch 50/250, Loss: 1.8268\n",
      "  Batch 60/250, Loss: 1.7670\n",
      "  Batch 70/250, Loss: 1.5565\n",
      "  Batch 80/250, Loss: 1.4967\n",
      "  Batch 90/250, Loss: 1.4369\n",
      "  Batch 100/250, Loss: 1.3676\n",
      "  Batch 110/250, Loss: 1.3843\n",
      "  Batch 120/250, Loss: 1.2563\n",
      "  Batch 130/250, Loss: 1.2416\n",
      "  Batch 140/250, Loss: 1.1141\n",
      "  Batch 150/250, Loss: 1.1054\n",
      "  Batch 160/250, Loss: 0.9457\n",
      "  Batch 170/250, Loss: 1.2250\n",
      "  Batch 180/250, Loss: 0.9338\n",
      "  Batch 190/250, Loss: 1.1272\n",
      "  Batch 200/250, Loss: 1.0451\n",
      "  Batch 210/250, Loss: 1.0058\n",
      "  Batch 220/250, Loss: 0.9245\n",
      "  Batch 230/250, Loss: 0.8964\n",
      "  Batch 240/250, Loss: 0.9835\n",
      "  Batch 250/250, Loss: 1.0173\n",
      "Epoch 1/3 - Evaluating...\n",
      "[001/3] train_loss=1.3947 val_loss=0.8655 val_macroF1=0.9466 lr=5.00e-04 time=9830.4s\n",
      "New best F1: 0.9466, saved to resnet101_best.pth\n",
      "Epoch 2/3 - Training...\n",
      "  Batch 10/250, Loss: 0.9754\n",
      "  Batch 20/250, Loss: 0.8974\n",
      "  Batch 30/250, Loss: 0.9514\n",
      "  Batch 40/250, Loss: 0.9353\n",
      "  Batch 50/250, Loss: 0.9353\n",
      "  Batch 60/250, Loss: 0.8488\n",
      "  Batch 70/250, Loss: 0.9040\n",
      "  Batch 80/250, Loss: 0.8590\n",
      "  Batch 90/250, Loss: 0.9379\n",
      "  Batch 100/250, Loss: 0.9134\n",
      "  Batch 110/250, Loss: 0.9428\n",
      "  Batch 120/250, Loss: 0.8626\n",
      "  Batch 130/250, Loss: 0.8553\n",
      "  Batch 140/250, Loss: 0.8539\n",
      "  Batch 150/250, Loss: 0.8552\n",
      "  Batch 160/250, Loss: 0.8335\n",
      "  Batch 170/250, Loss: 0.9362\n",
      "  Batch 180/250, Loss: 0.8217\n",
      "  Batch 190/250, Loss: 0.8471\n",
      "  Batch 200/250, Loss: 0.8017\n",
      "  Batch 210/250, Loss: 0.8362\n",
      "  Batch 220/250, Loss: 0.8019\n",
      "  Batch 230/250, Loss: 0.8080\n",
      "  Batch 240/250, Loss: 0.7907\n",
      "  Batch 250/250, Loss: 0.8622\n",
      "Epoch 2/3 - Evaluating...\n",
      "[002/3] train_loss=0.8769 val_loss=0.7783 val_macroF1=0.9612 lr=2.50e-04 time=9859.2s\n",
      "New best F1: 0.9612, saved to resnet101_best.pth\n",
      "Epoch 3/3 - Training...\n",
      "  Batch 10/250, Loss: 0.7386\n",
      "  Batch 20/250, Loss: 0.9015\n",
      "  Batch 30/250, Loss: 0.7727\n",
      "  Batch 40/250, Loss: 0.7699\n",
      "  Batch 50/250, Loss: 0.8267\n",
      "  Batch 60/250, Loss: 0.8616\n",
      "  Batch 70/250, Loss: 0.7918\n",
      "  Batch 80/250, Loss: 0.7505\n",
      "  Batch 90/250, Loss: 0.7910\n",
      "  Batch 100/250, Loss: 0.7224\n",
      "  Batch 110/250, Loss: 0.7932\n",
      "  Batch 120/250, Loss: 0.8181\n",
      "  Batch 130/250, Loss: 0.8411\n",
      "  Batch 140/250, Loss: 0.8426\n",
      "  Batch 150/250, Loss: 0.7166\n",
      "  Batch 160/250, Loss: 0.7618\n",
      "  Batch 170/250, Loss: 0.8918\n",
      "  Batch 180/250, Loss: 0.8021\n",
      "  Batch 190/250, Loss: 0.7624\n",
      "  Batch 200/250, Loss: 0.8706\n",
      "  Batch 210/250, Loss: 0.7623\n",
      "  Batch 220/250, Loss: 0.7784\n",
      "  Batch 230/250, Loss: 0.8627\n",
      "  Batch 240/250, Loss: 0.7656\n",
      "  Batch 250/250, Loss: 0.8161\n",
      "Epoch 3/3 - Evaluating...\n",
      "[003/3] train_loss=0.8125 val_loss=0.7627 val_macroF1=0.9652 lr=0.00e+00 time=10073.4s\n",
      "New best F1: 0.9652, saved to resnet101_best.pth\n",
      "Training completed!\n",
      "Best F1: 0.9652, saved to resnet101_best.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    # (에폭 전환 시점에 동결 해제)\n",
    "    if epoch == FREEZE_WARMUP_EPOCHS:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WD)\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    train_loss, n = 0.0, 0\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Training...\")\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=DEVICE):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "        \n",
    "        # 진행 상황 출력 (매 10배치마다)\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Evaluating...\")\n",
    "    val_loss, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[{epoch+1:03d}/{EPOCHS}] \"\n",
    "          f\"train_loss={train_loss/n:.4f} val_loss={val_loss:.4f} val_macroF1={val_f1:.4f} \"\n",
    "          f\"lr={scheduler.get_last_lr()[0]:.2e} time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch+1, \"f1\": best_f1}, best_path)\n",
    "        print(f\"New best F1: {best_f1:.4f}, saved to {best_path}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best F1: {best_f1:.4f}, saved to {best_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916102dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taeya_python_env3.13",
   "language": "python",
   "name": "taeya_python_env3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
